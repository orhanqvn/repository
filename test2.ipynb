{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test2",
      "provenance": [],
      "authorship_tag": "ABX9TyPiJVNc0l9yxlXQ0ZnUSJwy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orhanqvn/repository/blob/master/test2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS8omB3e6RUq"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQVq7R_AKR5Z"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('intents.json') as file:\n",
        "    intents = json.load(file, strict = False)\n",
        "intents = intents['intents']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JshkGgXlKXfH"
      },
      "source": [
        "print(\"[\", end = \"\")\n",
        "for intent in intents:\n",
        "  print(\"{\", end = \"\")\n",
        "  for key, value in intent.items():\n",
        "    print(\"{}: {},\".format(key, value))\n",
        "  print(\"\\b\\b\\n},\")\n",
        "print(\"\\b\\b]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d26OM7gORMVn"
      },
      "source": [
        "!pip install tensorflow==1.15\n",
        "!pip install tflearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xMLi5_rRTzk"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import tflearn\n",
        "import random\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsr1imxiRYOF"
      },
      "source": [
        "import nltk\n",
        "nltk.download('all')\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer('french')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tWywVlUReNU"
      },
      "source": [
        "retrain_model = True\n",
        "\n",
        "if retrain_model:\n",
        "    all_words = [] \n",
        "    all_tags = [] \n",
        "    intent_patterns = [] \n",
        "    intent_tags = [] \n",
        "    \n",
        "    for intent in intents:\n",
        "        for pattern in intent['patterns']:\n",
        "            words = nltk.word_tokenize(pattern)\n",
        "\n",
        "            all_words.extend(words)\n",
        "            intent_patterns.append(words)\n",
        "            intent_tags.append(intent['tag'])\n",
        "            \n",
        "        all_tags.append(intent['tag'])\n",
        "      \n",
        "    all_words = [stemmer.stem(word.lower()) for word in all_words]\n",
        "    all_words = sorted(list(set(all_words)))\n",
        "    \n",
        "    all_tags = sorted(all_tags)\n",
        "    \n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    \n",
        "    y_empty = [0 for i in range(len(all_tags))]\n",
        "     \n",
        "    for index, intent in enumerate(intent_patterns):\n",
        "        bag_of_words = []\n",
        "        \n",
        "        intent_words = [stemmer.stem(word.lower()) for word in intent]\n",
        "        \n",
        "        for word in all_words:\n",
        "            if word in intent_words:\n",
        "                bag_of_words.append(1)\n",
        "            else:\n",
        "                bag_of_words.append(0)\n",
        "                \n",
        "        one_hot_encode_y = y_empty[:]\n",
        "        one_hot_encode_y[all_tags.index(intent_tags[index])] = 1\n",
        "        \n",
        "        x_train.append(bag_of_words)\n",
        "        y_train.append(one_hot_encode_y)\n",
        "    \n",
        "    x_train = np.array(x_train)\n",
        "    y_train = np.array(y_train)\n",
        "    \n",
        "    with open('training_data.pickle', 'wb') as f:\n",
        "        pickle.dump((all_words, all_tags, x_train, y_train), f)\n",
        "else:\n",
        "    with open('training_data.pickle', 'rb') as f:\n",
        "        all_words, all_tags, x_train, y_train = pickle.load(f)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJaoyIfvR0e9"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "neural_net = tflearn.input_data(shape = [None, len(x_train[0])])\n",
        "neural_net = tflearn.fully_connected(neural_net, 8)\n",
        "neural_net = tflearn.fully_connected(neural_net, 8)\n",
        "neural_net = tflearn.fully_connected(neural_net, len(y_train[0]), activation = 'softmax')\n",
        "neural_net = tflearn.regression(neural_net)\n",
        "\n",
        "model = tflearn.DNN(neural_net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saX-q3yqR6Vk"
      },
      "source": [
        "if retrain_model:\n",
        "    model.fit(x_train, y_train, n_epoch = 500, batch_size = 8, show_metric = True)\n",
        "    model.save('model.tfl')\n",
        "else:\n",
        "    model.load('./model.tfl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hVpm4yCSASU"
      },
      "source": [
        "def text_to_bag(text, all_words):\n",
        "    bag_of_words = [0 for i in range(len(all_words))]\n",
        "    \n",
        "    text_words = nltk.word_tokenize(text)\n",
        "    text_words = [stemmer.stem(word.lower()) for word in text_words]\n",
        "    \n",
        "    for word in text_words:\n",
        "        if word in all_words:\n",
        "            bag_of_words[all_words.index(word)] = 1\n",
        "    \n",
        "    return np.array(bag_of_words)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77txFNE3SHbN"
      },
      "source": [
        "def chat():\n",
        "    print(\"Entrez un message pour parler au bot [type quit to exit].\")\n",
        "    \n",
        "    context_state = None\n",
        "    \n",
        "    default_responses = ['Désolé, je ne suis pas sûr de savoir ce que vous voulez dire ! Vous pouvez essayer de reformuler cela ou de dire autre chose !',\n",
        "                         'Je ne comprends pas ça! Essayez de reformuler ou de dire autre chose.']\n",
        "\n",
        "    while True:\n",
        "        user_chat = str(input('You: '))\n",
        "        if user_chat.lower() == 'quit':\n",
        "            break\n",
        "        \n",
        "        user_chat_bag = text_to_bag(user_chat, all_words)\n",
        "\n",
        "        response = model.predict([user_chat_bag])[0]\n",
        "\n",
        "        response_index = np.argmax(response)\n",
        "        response_tag = all_tags[response_index]\n",
        "        \n",
        "        if response[response_index] > 0.8:\n",
        "            for intent in intents:\n",
        "                if intent['tag'] == response_tag:\n",
        "                    if 'context_filter' not in intent or 'context_filter' in intent and intent['context_filter'] == context_state:\n",
        "                        possible_responses = intent['responses']\n",
        "                        if 'context_set' in intent:\n",
        "                            context_state = intent['context_set']\n",
        "                        else:\n",
        "                            context_state = None\n",
        "                        print(random.choice(possible_responses))\n",
        "                    else:\n",
        "                        print(random.choice(default_responses))\n",
        "        else:\n",
        "            print(random.choice(default_responses))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY_lr9knSehe"
      },
      "source": [
        "chat()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}